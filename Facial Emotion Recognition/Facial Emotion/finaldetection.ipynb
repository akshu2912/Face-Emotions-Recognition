{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41846e9d-4ab7-4285-ac4c-2fec169bb92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53df71d4-f9b6-48ed-b60f-2ce062d0425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17df20b4-465d-4d46-88e6-d84e519f5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949e4a4d-6ef8-41da-b34c-721c3dda3a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cbadb03-676b-43ec-9a97-9cdf45ac4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                image     label\n",
      "0            images/train\\angry\\0.jpg     angry\n",
      "1            images/train\\angry\\1.jpg     angry\n",
      "2           images/train\\angry\\10.jpg     angry\n",
      "3        images/train\\angry\\10002.jpg     angry\n",
      "4        images/train\\angry\\10016.jpg     angry\n",
      "...                               ...       ...\n",
      "28816  images/train\\surprise\\9969.jpg  surprise\n",
      "28817  images/train\\surprise\\9985.jpg  surprise\n",
      "28818  images/train\\surprise\\9990.jpg  surprise\n",
      "28819  images/train\\surprise\\9992.jpg  surprise\n",
      "28820  images/train\\surprise\\9996.jpg  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a7b03cc-e0c4-451e-bc46-46e46f22b5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd4278f-fe87-475d-87f1-a3f71fc42064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              image     label\n",
      "0       images/test\\angry\\10052.jpg     angry\n",
      "1       images/test\\angry\\10065.jpg     angry\n",
      "2       images/test\\angry\\10079.jpg     angry\n",
      "3       images/test\\angry\\10095.jpg     angry\n",
      "4       images/test\\angry\\10121.jpg     angry\n",
      "...                             ...       ...\n",
      "7061  images/test\\surprise\\9806.jpg  surprise\n",
      "7062  images/test\\surprise\\9830.jpg  surprise\n",
      "7063  images/test\\surprise\\9853.jpg  surprise\n",
      "7064  images/test\\surprise\\9878.jpg  surprise\n",
      "7065   images/test\\surprise\\993.jpg  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n",
      "0         images/test\\angry\\10052.jpg\n",
      "1         images/test\\angry\\10065.jpg\n",
      "2         images/test\\angry\\10079.jpg\n",
      "3         images/test\\angry\\10095.jpg\n",
      "4         images/test\\angry\\10121.jpg\n",
      "                    ...              \n",
      "7061    images/test\\surprise\\9806.jpg\n",
      "7062    images/test\\surprise\\9830.jpg\n",
      "7063    images/test\\surprise\\9853.jpg\n",
      "7064    images/test\\surprise\\9878.jpg\n",
      "7065     images/test\\surprise\\993.jpg\n",
      "Name: image, Length: 7066, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c60ae0a-5ef5-4e7b-9cf8-2d8a74c1db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70e1b090-07bc-4fa4-a21f-1505eccfd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image,grayscale =  True )\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features),48,48,1)\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e913b0d-0c4d-4dd2-933b-e7788b1256e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83aa809782804b4f8dca374f5ca736f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_preprocessing\\image\\utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "train_features = extract_features(train['image']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9504596f-9901-48ac-b136-665f2c003420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0797fa930674a60bc5575b44d6e23f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd74169-0a34-4689-8486-4041791736e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbbe06a7-e8ba-4859-8d18-b50dc6b26fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e71fcd3f-3aef-4675-9d05-977a65641385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6f9ab0d-81ee-4349-9c0a-0537aeda53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d1c3dd2-0758-4b2e-8860-6bcf1fa7cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,num_classes = 7)\n",
    "y_test = to_categorical(y_test,num_classes = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c35e097-1812-46c7-b70d-c3e79631bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# convolutional layers\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "# fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01283860-e1ca-4cf5-924a-2d6c2f8b108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'accuracy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a6a1dd6-a215-472f-8c04-29cdf44df7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "226/226 [==============================] - 80s 343ms/step - loss: 1.8278 - accuracy: 0.2430 - val_loss: 1.8127 - val_accuracy: 0.2583\n",
      "Epoch 2/50\n",
      "226/226 [==============================] - 77s 342ms/step - loss: 1.8119 - accuracy: 0.2482 - val_loss: 1.7867 - val_accuracy: 0.2614\n",
      "Epoch 3/50\n",
      "226/226 [==============================] - 77s 340ms/step - loss: 1.7655 - accuracy: 0.2654 - val_loss: 1.6863 - val_accuracy: 0.3051\n",
      "Epoch 4/50\n",
      "226/226 [==============================] - 77s 340ms/step - loss: 1.6951 - accuracy: 0.3052 - val_loss: 1.6251 - val_accuracy: 0.3470\n",
      "Epoch 5/50\n",
      "226/226 [==============================] - 102s 450ms/step - loss: 1.6336 - accuracy: 0.3402 - val_loss: 1.5506 - val_accuracy: 0.4002\n",
      "Epoch 6/50\n",
      "226/226 [==============================] - 85s 377ms/step - loss: 1.5592 - accuracy: 0.3835 - val_loss: 1.4756 - val_accuracy: 0.4254\n",
      "Epoch 7/50\n",
      "226/226 [==============================] - 85s 375ms/step - loss: 1.5058 - accuracy: 0.4145 - val_loss: 1.3771 - val_accuracy: 0.4772\n",
      "Epoch 8/50\n",
      "226/226 [==============================] - 84s 374ms/step - loss: 1.4668 - accuracy: 0.4311 - val_loss: 1.3496 - val_accuracy: 0.4863\n",
      "Epoch 9/50\n",
      "226/226 [==============================] - 84s 373ms/step - loss: 1.4423 - accuracy: 0.4411 - val_loss: 1.3338 - val_accuracy: 0.4871\n",
      "Epoch 10/50\n",
      "226/226 [==============================] - 85s 374ms/step - loss: 1.4189 - accuracy: 0.4536 - val_loss: 1.2921 - val_accuracy: 0.5068\n",
      "Epoch 11/50\n",
      "226/226 [==============================] - 85s 375ms/step - loss: 1.4006 - accuracy: 0.4603 - val_loss: 1.2932 - val_accuracy: 0.5055\n",
      "Epoch 12/50\n",
      "226/226 [==============================] - 84s 374ms/step - loss: 1.3902 - accuracy: 0.4617 - val_loss: 1.2700 - val_accuracy: 0.5224\n",
      "Epoch 13/50\n",
      "226/226 [==============================] - 82s 365ms/step - loss: 1.3756 - accuracy: 0.4724 - val_loss: 1.2770 - val_accuracy: 0.5116\n",
      "Epoch 14/50\n",
      "226/226 [==============================] - 76s 337ms/step - loss: 1.3673 - accuracy: 0.4755 - val_loss: 1.2655 - val_accuracy: 0.5110\n",
      "Epoch 15/50\n",
      "226/226 [==============================] - 74s 329ms/step - loss: 1.3542 - accuracy: 0.4795 - val_loss: 1.2590 - val_accuracy: 0.5178\n",
      "Epoch 16/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.3505 - accuracy: 0.4816 - val_loss: 1.2173 - val_accuracy: 0.5323\n",
      "Epoch 17/50\n",
      "226/226 [==============================] - 76s 337ms/step - loss: 1.3370 - accuracy: 0.4894 - val_loss: 1.2172 - val_accuracy: 0.5395\n",
      "Epoch 18/50\n",
      "226/226 [==============================] - 75s 334ms/step - loss: 1.3285 - accuracy: 0.4920 - val_loss: 1.2010 - val_accuracy: 0.5416\n",
      "Epoch 19/50\n",
      "226/226 [==============================] - 76s 336ms/step - loss: 1.3235 - accuracy: 0.4923 - val_loss: 1.2025 - val_accuracy: 0.5427\n",
      "Epoch 20/50\n",
      "226/226 [==============================] - 75s 333ms/step - loss: 1.3123 - accuracy: 0.4966 - val_loss: 1.2015 - val_accuracy: 0.5419\n",
      "Epoch 21/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.3054 - accuracy: 0.4998 - val_loss: 1.1948 - val_accuracy: 0.5403\n",
      "Epoch 22/50\n",
      "226/226 [==============================] - 75s 333ms/step - loss: 1.3059 - accuracy: 0.5012 - val_loss: 1.1939 - val_accuracy: 0.5492\n",
      "Epoch 23/50\n",
      "226/226 [==============================] - 76s 335ms/step - loss: 1.2927 - accuracy: 0.5047 - val_loss: 1.1824 - val_accuracy: 0.5500\n",
      "Epoch 24/50\n",
      "226/226 [==============================] - 75s 333ms/step - loss: 1.2882 - accuracy: 0.5077 - val_loss: 1.1710 - val_accuracy: 0.5552\n",
      "Epoch 25/50\n",
      "226/226 [==============================] - 76s 335ms/step - loss: 1.2813 - accuracy: 0.5109 - val_loss: 1.1756 - val_accuracy: 0.5613\n",
      "Epoch 26/50\n",
      "226/226 [==============================] - 75s 330ms/step - loss: 1.2798 - accuracy: 0.5091 - val_loss: 1.1656 - val_accuracy: 0.5552\n",
      "Epoch 27/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.2725 - accuracy: 0.5133 - val_loss: 1.1753 - val_accuracy: 0.5528\n",
      "Epoch 28/50\n",
      "226/226 [==============================] - 77s 339ms/step - loss: 1.2689 - accuracy: 0.5151 - val_loss: 1.1703 - val_accuracy: 0.5590\n",
      "Epoch 29/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.2636 - accuracy: 0.5157 - val_loss: 1.1603 - val_accuracy: 0.5593\n",
      "Epoch 30/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.2593 - accuracy: 0.5215 - val_loss: 1.1678 - val_accuracy: 0.5541\n",
      "Epoch 31/50\n",
      "226/226 [==============================] - 75s 333ms/step - loss: 1.2563 - accuracy: 0.5190 - val_loss: 1.1527 - val_accuracy: 0.5643\n",
      "Epoch 32/50\n",
      "226/226 [==============================] - 75s 333ms/step - loss: 1.2509 - accuracy: 0.5234 - val_loss: 1.1555 - val_accuracy: 0.5604\n",
      "Epoch 33/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.2532 - accuracy: 0.5214 - val_loss: 1.1483 - val_accuracy: 0.5640\n",
      "Epoch 34/50\n",
      "226/226 [==============================] - 75s 331ms/step - loss: 1.2530 - accuracy: 0.5212 - val_loss: 1.1491 - val_accuracy: 0.5662\n",
      "Epoch 35/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.2472 - accuracy: 0.5270 - val_loss: 1.1492 - val_accuracy: 0.5647\n",
      "Epoch 36/50\n",
      "226/226 [==============================] - 358s 2s/step - loss: 1.2375 - accuracy: 0.5293 - val_loss: 1.1346 - val_accuracy: 0.5705\n",
      "Epoch 37/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.2406 - accuracy: 0.5274 - val_loss: 1.1404 - val_accuracy: 0.5727\n",
      "Epoch 38/50\n",
      "226/226 [==============================] - 76s 335ms/step - loss: 1.2436 - accuracy: 0.5266 - val_loss: 1.1401 - val_accuracy: 0.5730\n",
      "Epoch 39/50\n",
      "226/226 [==============================] - 75s 330ms/step - loss: 1.2329 - accuracy: 0.5298 - val_loss: 1.1377 - val_accuracy: 0.5651\n",
      "Epoch 40/50\n",
      "226/226 [==============================] - 76s 334ms/step - loss: 1.2349 - accuracy: 0.5287 - val_loss: 1.1345 - val_accuracy: 0.5746\n",
      "Epoch 41/50\n",
      "226/226 [==============================] - 75s 331ms/step - loss: 1.2327 - accuracy: 0.5308 - val_loss: 1.1379 - val_accuracy: 0.5713\n",
      "Epoch 42/50\n",
      "226/226 [==============================] - 75s 334ms/step - loss: 1.2332 - accuracy: 0.5307 - val_loss: 1.1314 - val_accuracy: 0.5750\n",
      "Epoch 43/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.2195 - accuracy: 0.5357 - val_loss: 1.1312 - val_accuracy: 0.5733\n",
      "Epoch 44/50\n",
      "226/226 [==============================] - 75s 333ms/step - loss: 1.2209 - accuracy: 0.5340 - val_loss: 1.1283 - val_accuracy: 0.5754\n",
      "Epoch 45/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.2165 - accuracy: 0.5360 - val_loss: 1.1145 - val_accuracy: 0.5808\n",
      "Epoch 46/50\n",
      "226/226 [==============================] - 75s 332ms/step - loss: 1.2253 - accuracy: 0.5305 - val_loss: 1.1350 - val_accuracy: 0.5664\n",
      "Epoch 47/50\n",
      "226/226 [==============================] - 75s 333ms/step - loss: 1.2132 - accuracy: 0.5388 - val_loss: 1.1204 - val_accuracy: 0.5759\n",
      "Epoch 48/50\n",
      "226/226 [==============================] - 76s 338ms/step - loss: 1.2099 - accuracy: 0.5391 - val_loss: 1.1100 - val_accuracy: 0.5807\n",
      "Epoch 49/50\n",
      "226/226 [==============================] - 75s 334ms/step - loss: 1.2144 - accuracy: 0.5389 - val_loss: 1.1266 - val_accuracy: 0.5757\n",
      "Epoch 50/50\n",
      "226/226 [==============================] - 75s 333ms/step - loss: 1.2125 - accuracy: 0.5411 - val_loss: 1.1235 - val_accuracy: 0.5838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28e757ecd10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x= x_train,y = y_train, batch_size = 128, epochs = 50, validation_data = (x_test,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265c8975-0cbc-4243-903f-345a8a13f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ff925f-5bc4-4454-8bdd-0c4b826d1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe497592-611b-480d-99ba-5f72dc56694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(\"facialemotionmodel.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"facialemotionmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c01452f6-0b35-4a42-a6c8-95948b9eca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7809b90a-09f9-419c-a3dd-0be3c2779385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(image):\n",
    "    img = load_img(image,grayscale =  True )\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1,48,48,1)\n",
    "    return feature/255.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf6e2ed-49d7-47af-8884-811614f0ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of sad\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/train/sad/42.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal image is of sad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[0;32m      5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mef\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mef\u001b[39m(image):\n\u001b[1;32m----> 2\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m(image,grayscale \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[0;32m      3\u001b[0m     feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m      4\u001b[0m     feature \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_img' is not defined"
     ]
    }
   ],
   "source": [
    "image = 'images/train/sad/42.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5c6a08-dbfa-4a60-8aa8-6a3c23e0e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e68653fc-b675-4ebe-90ad-327af7bc5e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of happy\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/train/happy/7.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal image is of happy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[0;32m      5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mef\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mef\u001b[39m(image):\n\u001b[1;32m----> 2\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m(image,grayscale \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[0;32m      3\u001b[0m     feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m      4\u001b[0m     feature \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_img' is not defined"
     ]
    }
   ],
   "source": [
    "image = 'images/train/happy/7.jpg'\n",
    "print(\"original image is of happy\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd76197-384b-4a72-9b77-2d74af4c25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'images/train/fear/2.jpg'\n",
    "print(\"original image is of fear\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474cfd81-758d-4b47-89d8-302d601e4c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'images/train/fear/17.jpg'\n",
    "print(\"original image is of fear\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e303a3-3118-4a20-a561-8960b1f60633",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'images/train/happy/7.jpg'\n",
    "print(\"original image is of happy\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac75e8b-65af-4712-8120-07e0bafce42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of neutral\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/train/neutral/11.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal image is of neutral\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[0;32m      5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mef\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mef\u001b[39m(image):\n\u001b[1;32m----> 2\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m(image,grayscale \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[0;32m      3\u001b[0m     feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m      4\u001b[0m     feature \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_img' is not defined"
     ]
    }
   ],
   "source": [
    "image = 'images/train/neutral/11.jpg'\n",
    "print(\"original image is of neutral\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9581b152-ee6c-4c38-9f6d-48cbefca753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of surprise\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/train/surprise/61.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal image is of surprise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[0;32m      5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mef\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mef\u001b[39m(image):\n\u001b[1;32m----> 2\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m(image,grayscale \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[0;32m      3\u001b[0m     feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m      4\u001b[0m     feature \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_img' is not defined"
     ]
    }
   ],
   "source": [
    "image = 'images/train/surprise/61.jpg'\n",
    "print(\"original image is of surprise\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e02a2e0-a2d7-4c88-a783-684d4b803a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of angry\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/train/angry/76.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal image is of angry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img)\n\u001b[0;32m      5\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m label[pred\u001b[38;5;241m.\u001b[39margmax()]\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mef\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mef\u001b[39m(image):\n\u001b[1;32m----> 2\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m(image,grayscale \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[0;32m      3\u001b[0m     feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m      4\u001b[0m     feature \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m48\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_img' is not defined"
     ]
    }
   ],
   "source": [
    "image = 'images/train/angry/76.jpg'\n",
    "print(\"original image is of angry\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f507e4-0847-4ffd-a467-413a99ee30c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
